Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks which require world or domain knowledge \cite{petroni2021kilt_LR}.  However, despite their potential and recent advancements, these models face a concerning issue, “hallucination”, a phenomenon where the model generates plausible-sounding but unfaithful or nonsensical information \cite{ji2023survey}.

 Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38.

Large language models(LLMs) are now regarded as one of the most advancing fields in artificial intelligence researches (OpenAI, 2023; Chiang et al., 2023; Taori et al., 2023; Touvron et al., 2023).

OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.



\subsection{Why Evaluation Matters}
\input{Chapter/LiteratureReview/WhyEvaMatter}