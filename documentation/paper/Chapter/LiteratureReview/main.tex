Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks which require world or domain knowledge \cite{petroni2021kilt_LR}.  However, despite their potential and recent advancements, these models face a concerning issue, “hallucination”, a phenomenon where the model generates plausible-sounding but unfaithful or nonsensical information \cite{ji2023survey}.


\subsection{introduction}