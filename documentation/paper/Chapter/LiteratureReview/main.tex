Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks which require world or domain knowledge \cite{petroni2021kilt_LR}.  However, despite their potential and recent advancements, these models face a concerning issue, “hallucination”, a phenomenon where the model generates plausible-sounding but unfaithful or nonsensical information \cite{ji2023survey}.



 Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput
ing Surveys, 55(12):1–38.

Large language models(LLMs) are now regarded as one of the most advancing fields in artificial intelligence researches (OpenAI, 2023; Chiang et al., 2023; Taori et al., 2023; Touvron et al., 2023). By sufficiently pre-training on massive textual corpus, and carefully fine-tuning and aligning on high-quality instruction-following data, LLMs have demonstrated remarkable capabilities, e.g. understanding human instructions and generating helpful responses (Wei et al., 2021; Ouyang et al., 2022; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023). However, the robustness of current LLMs, even those leading ones, is still far from promising in recent literature (Gu et al., 2022; Sun et al., 2023; Liang et al., 2023).

OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.



\subsection{First Object}
\input{Chapter/LiteratureReview/FirstObject}