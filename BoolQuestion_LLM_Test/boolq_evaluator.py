#!/usr/bin/env python3
"""
BoolQ LLMè¯„ä¼°å®éªŒ - ä¸»å…¥å£æ–‡ä»¶

åŠŸèƒ½:
- å¼‚æ­¥å¹¶å‘è¯„ä¼°å¤šä¸ªLLMæ¨¡å‹çš„äºŒå€¼åˆ†ç±»èƒ½åŠ›
- æ”¯æŒå¤šç§æç¤ºè¯é£æ ¼å’Œè¾“å‡ºæ ¼å¼
- æ”¯æŒlogprobsåˆ†æ
- å®æ—¶è¿›åº¦å±•ç¤º
- Tokenä½¿ç”¨é‡å’Œæˆæœ¬ç»Ÿè®¡

ä½¿ç”¨ç¤ºä¾‹:
    python main.py --model google/gemini-2.0-flash-001 --style sse --limit 100
    python main.py --model deepseek-v3-250324 --style direct --concurrency 20
"""
from __future__ import annotations

import argparse
import asyncio
import random
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

from loguru import logger
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn, TaskProgressColumn

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import ExperimentConfig
from utils.models_utils import BoolQItem, EvaluationResult, ExperimentStats, PromptStyle, TokenUsage, CostInfo, EvalMode
from llm_client import create_llm_client, BaseLLMClient
from prompt_manager import PromptManager
from utils import DatasetManager, ExperimentLogger, AsyncResultWriter, ResponseParser, StatisticsHelper
from i18n import t, set_language, get_language


class BoolQEvaluator:
    """
    BoolQå¼‚æ­¥è¯„ä¼°å™¨
    
    è´Ÿè´£:
    - ç®¡ç†å¼‚æ­¥è¯„ä¼°ä»»åŠ¡
    - åè°ƒå„ç»„ä»¶å·¥ä½œ
    - æ”¶é›†å’Œç»Ÿè®¡ç»“æœ
    """
    
    def __init__(self, config: ExperimentConfig, console: Console):
        self.config = config
        self.console = console
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger_manager = ExperimentLogger(
            output_dir=config.output_dir,
            model_name=config.model,
            style=config.style.value,
            use_reasoning=config.use_reasoning,
            reason_order=config.reason_order.value,
            console=console,
        )
        
        self.dataset_manager = DatasetManager(
            split=config.split,
            limit=config.limit,
            dirty_data_path=config.dirty_data_path,
        )
        
        self.prompt_manager = PromptManager(
            style=config.style,
            eval_mode=config.eval_mode,
            use_reasoning=config.use_reasoning,
            reason_order=config.reason_order,
        )
        
        self.response_parser = ResponseParser(style=config.style)
        
        self.llm_client = create_llm_client(config.get_llm_config())
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = ExperimentStats()
        
        # å¼‚æ­¥å†™å…¥å™¨
        self.result_writer: Optional[AsyncResultWriter] = None
        
        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self._semaphore: Optional[asyncio.Semaphore] = None
        
        # ä»»åŠ¡è®¡æ•°å™¨
        self._processing_count = 0  # æ­£åœ¨å¤„ç†ä¸­çš„ä»»åŠ¡æ•°
        self._completed_count = 0   # å·²å®Œæˆçš„ä»»åŠ¡æ•°
        self._counter_lock = asyncio.Lock()
    
    async def _increment_processing(self) -> None:
        """å¢åŠ å¤„ç†ä¸­è®¡æ•°"""
        async with self._counter_lock:
            self._processing_count += 1
    
    async def _decrement_processing(self) -> None:
        """å‡å°‘å¤„ç†ä¸­è®¡æ•°ï¼Œå¢åŠ å®Œæˆè®¡æ•°"""
        async with self._counter_lock:
            self._processing_count -= 1
            self._completed_count += 1
    
    # è§£æå¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
    PARSE_RETRY_COUNT = 2
    
    async def evaluate_single(
        self,
        item: BoolQItem,
        reversal: bool,
    ) -> EvaluationResult:
        """
        è¯„ä¼°å•æ¡æ•°æ®ï¼ˆå¸¦è§£æå¤±è´¥é‡è¯•ï¼‰
        
        Args:
            item: BoolQæ•°æ®é¡¹
            reversal: æ˜¯å¦åè½¬é¢„æœŸç­”æ¡ˆ (ä»…åœ¨validateæ¨¡å¼ä¸‹æœ‰æ•ˆ)
            
        Returns:
            EvaluationResult: è¯„ä¼°ç»“æœ
        """
        # æ ¹æ®è¯„ä¼°æ¨¡å¼å‡†å¤‡prompt
        if self.config.eval_mode == EvalMode.ANSWER:
            # answeræ¨¡å¼ï¼šä¸éœ€è¦preset_answerï¼Œä¸ä½¿ç”¨reversal
            prompt = self.prompt_manager.create_prompt(
                question=item.question,
                passage=item.passage,
            )
            # answeræ¨¡å¼ä¸‹reversalæ— æ•ˆ
            reversal = False
        else:
            # validateæ¨¡å¼ï¼šéœ€è¦preset_answer
            preset_answer = item.answer if not reversal else (not item.answer)
            prompt = self.prompt_manager.create_prompt(
                question=item.question,
                passage=item.passage,
                preset_answer=preset_answer,
            )
        
        await self._increment_processing()
        
        # ç´¯è®¡tokenå’Œæˆæœ¬ï¼ˆç”¨äºå¤šæ¬¡é‡è¯•æ—¶åˆå¹¶ç»Ÿè®¡ï¼‰
        total_latency = 0.0
        last_llm_response = None
        last_parse_result = None
        last_error = None
        
        try:
            # è§£æå¤±è´¥é‡è¯•å¾ªç¯
            for attempt in range(self.PARSE_RETRY_COUNT + 1):
                try:
                    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
                    async with self._semaphore:
                        llm_response = await self.llm_client.generate(prompt)
                    
                    last_llm_response = llm_response
                    total_latency += llm_response.latency
                    
                    # è§£æå“åº”
                    parse_result = self.response_parser.parse(llm_response.content)
                    last_parse_result = parse_result
                    
                    # è§£ææˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    if parse_result.success:
                        break
                    
                    # è§£æå¤±è´¥ï¼Œå¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œè®°å½•æ—¥å¿—å¹¶é‡è¯•
                    if attempt < self.PARSE_RETRY_COUNT:
                        logger.warning(
                            f"è§£æå¤±è´¥ (index={item.index}, å°è¯• {attempt + 1}/{self.PARSE_RETRY_COUNT + 1}): "
                            f"{parse_result.error_message[:80]}... é‡è¯•ä¸­"
                        )
                    
                except Exception as e:
                    last_error = e
                    # APIé”™è¯¯ï¼Œå¦‚æœè¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œè®°å½•å¹¶é‡è¯•
                    if attempt < self.PARSE_RETRY_COUNT:
                        logger.warning(
                            f"APIé”™è¯¯ (index={item.index}, å°è¯• {attempt + 1}/{self.PARSE_RETRY_COUNT + 1}): "
                            f"{type(e).__name__}: {str(e)[:80]}... é‡è¯•ä¸­"
                        )
                    else:
                        raise  # æœ€åä¸€æ¬¡å°è¯•ä»ç„¶å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            # ä½¿ç”¨æœ€åä¸€æ¬¡çš„å“åº”ç»“æœ
            llm_response = last_llm_response
            parse_result = last_parse_result
            
            if llm_response is None or parse_result is None:
                raise last_error or Exception("æœªçŸ¥é”™è¯¯")
            
            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = False
            if parse_result.success and parse_result.answer is not None:
                if self.config.eval_mode == EvalMode.ANSWER:
                    # answeræ¨¡å¼ï¼šç›´æ¥æ¯”è¾ƒLLMå›ç­”ä¸åŸå§‹ç­”æ¡ˆ
                    is_correct = parse_result.answer == item.answer
                else:
                    # validateæ¨¡å¼ï¼šLLMå›ç­”Trueè¡¨ç¤ºéªŒè¯é€šè¿‡ï¼Œæ¯”è¾ƒæ˜¯å¦ä¸éåè½¬ä¸€è‡´
                    is_correct = parse_result.answer == (not reversal)
            
            result = EvaluationResult(
                status="success" if parse_result.success else "parse_error",
                question=item.question,
                passage=item.passage,
                expected=item.answer,
                is_reversal=reversal,
                predicted=parse_result.answer,
                is_correct=is_correct,
                raw_response=llm_response.content,
                parsed_reason=parse_result.reason,
                latency=total_latency,  # ä½¿ç”¨ç´¯è®¡å»¶è¿Ÿ
                timestamp=datetime.now().isoformat(),
                index=item.index,
                item_hash=item.hash,
                avg_logprobs=llm_response.avg_logprobs,
                confidence=llm_response.confidence,
                logprob_diff=llm_response.logprob_diff,
                logprobs=[lp.__dict__ for lp in (llm_response.logprobs or [])] if llm_response.logprobs else None,
                token_usage=llm_response.token_usage.to_dict() if llm_response.token_usage else None,
                cost_info=llm_response.cost_info.to_dict() if llm_response.cost_info else None,
                error=parse_result.error_message if not parse_result.success else None,
            )
            
            # è®°å½•å•é¡¹ç»“æœæ—¥å¿— (å¸¦é¢œè‰²)
            self.logger_manager.log_item_result(
                index=item.index,
                is_correct=is_correct,
                predicted=parse_result.answer,
                expected=item.answer,
                latency=total_latency,
                avg_logprobs=llm_response.avg_logprobs,
                token_usage=llm_response.token_usage,
                cost=llm_response.cost_info.total_cost if llm_response.cost_info else None,
            )
            
            return result
            
        except Exception as e:
            logger.error(f"è¯„ä¼°é”™è¯¯ (index={item.index}): {type(e).__name__}: {e}")
            return EvaluationResult(
                status="api_error",
                question=item.question,
                passage=item.passage,
                expected=item.answer,
                is_reversal=reversal,
                predicted=None,
                is_correct=False,
                raw_response=last_llm_response.content if last_llm_response else "",
                parsed_reason="",
                latency=total_latency,
                timestamp=datetime.now().isoformat(),
                index=item.index,
                item_hash=item.hash,
                error=str(e),
            )
        finally:
            await self._decrement_processing()
    
    async def run(self) -> None:
        """è¿è¡Œè¯„ä¼°"""
        # åŠ è½½æ•°æ®
        self.dataset_manager.load()
        total_items = len(self.dataset_manager)
        
        # æ˜¾ç¤ºè„æ•°æ®ç»Ÿè®¡
        dirty_stats = self.dataset_manager.dirty_stats
        if dirty_stats and dirty_stats.valid_records > 0:
            dirty_in_current = self.dataset_manager.dirty_count
            self.console.print(
                f"[dim]ğŸ“‹ {t('è„æ•°æ®: å·²åŠ è½½ {count} æ¡å“ˆå¸Œ, å½“å‰æ•°æ®é›†å°†è·³è¿‡ {skip} æ¡', count=len(self.dataset_manager._dirty_hashes), skip=dirty_in_current)}[/dim]"
            )
        
        # ä½¿ç”¨æ–°çš„æ—¥å¿—æ–¹æ³•
        self.logger_manager.log_evaluation_start(total_items, self.config.concurrency)
        
        # åˆå§‹åŒ–ä¿¡å·é‡
        self._semaphore = asyncio.Semaphore(self.config.concurrency)
        
        # åˆå§‹åŒ–å¼‚æ­¥å†™å…¥å™¨
        self.result_writer = AsyncResultWriter(self.logger_manager.data_path)
        await self.result_writer.start()
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡
        items = list(self.dataset_manager)
        
        # åˆ›å»ºè¿›åº¦æ¡ - æ”¹è¿›çš„æ˜¾ç¤º
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(bar_width=40),
            TaskProgressColumn(),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
            console=self.console,
            expand=False,
            refresh_per_second=10,
        ) as progress:
            
            task_id = progress.add_task(
                f"[cyan]è¯„ä¼° {self._get_short_model_name()}[/cyan]",
                total=len(items),
            )
            
            # åˆ›å»ºæ‰€æœ‰è¯„ä¼°ä»»åŠ¡
            async def evaluate_with_progress(item: BoolQItem) -> EvaluationResult:
                # answeræ¨¡å¼ä¸ä½¿ç”¨reversal
                if self.config.eval_mode == EvalMode.ANSWER:
                    reversal = False
                else:
                    reversal = random.random() < self.config.reversal_ratio
                result = await self.evaluate_single(item, reversal)
                
                # æ„å»ºtoken_usageå’Œcost_infoå¯¹è±¡
                token_usage = None
                cost_info = None
                if result.token_usage:
                    token_usage = TokenUsage(**result.token_usage)
                if result.cost_info:
                    cost_info = CostInfo(**result.cost_info)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats.update(
                    is_correct=result.is_correct,
                    parsed_successfully=(result.status == "success"),
                    avg_logprobs=result.avg_logprobs,
                    filter_threshold=self.config.filter_threshold,
                    token_usage=token_usage,
                    cost_info=cost_info,
                )
                
                # å†™å…¥ç»“æœ
                await self.result_writer.write(result.to_dict())
                
                # æ›´æ–°è¿›åº¦æ¡æè¿° - æ–°æ ¼å¼
                pending_write = self.result_writer.pending_count
                written = self.result_writer.written_count
                
                # Tokenå’Œæˆæœ¬ç»Ÿè®¡
                total_tokens = self.stats.total_token_usage.total_tokens
                total_cost = self.stats.total_cost.total_cost
                completed = self.stats.total  # å·²å®Œæˆçš„ä»»åŠ¡æ•°
                total_items_count = len(items)  # æ€»ä»»åŠ¡æ•°
                
                # è®¡ç®—é¢„ä¼°æ€»æˆæœ¬
                if completed > 0:
                    avg_cost_per_item = total_cost / completed
                    estimated_total_cost = avg_cost_per_item * total_items_count
                else:
                    estimated_total_cost = 0.0
                
                # æ ¼å¼åŒ–tokenæ•°ï¼ˆKè¡¨ç¤ºåƒï¼‰
                if total_tokens >= 1000:
                    token_str = f"{total_tokens/1000:.1f}K"
                else:
                    token_str = str(total_tokens)
                
                # æ ¼å¼åŒ–æˆæœ¬ï¼šå½“å‰æˆæœ¬ â†’ é¢„ä¼°æ€»æˆæœ¬
                def format_cost(cost: float) -> str:
                    if cost < 0.01:
                        return f"${cost:.4f}"
                    elif cost < 1:
                        return f"${cost:.3f}"
                    else:
                        return f"${cost:.2f}"
                
                cost_str = f"{format_cost(total_cost)}â†’{format_cost(estimated_total_cost)}"
                
                progress.update(
                    task_id,
                    advance=1,
                    description=(
                        f"[cyan]è¯„ä¼°[/cyan] | "
                        f"[green]Acc: {self.stats.accuracy:.1%}[/green] | "
                        f"[green]âœ“{self.stats.correct}[/green] "
                        f"[red]âœ—{self.stats.total - self.stats.correct}[/red] "
                        f"[yellow]âš {self.stats.errors}[/yellow] | "
                        f"[dim]tok:{token_str}[/dim] "
                        f"[cyan]{cost_str}[/cyan]"
                    ),
                )
                
                return result
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            tasks = [evaluate_with_progress(item) for item in items]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åœæ­¢å†™å…¥å™¨
        await self.result_writer.stop()
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        exception_count = 0
        for result in results:
            if isinstance(result, Exception):
                exception_count += 1
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {type(result).__name__}: {result}")
        
        if exception_count > 0:
            logger.warning(f"å…±æœ‰ {exception_count} ä¸ªä»»åŠ¡å‘ç”Ÿå¼‚å¸¸")
        
        # æ‰“å°æ€»ç»“
        self._print_summary()
    
    def _get_short_model_name(self) -> str:
        """è·å–ç®€çŸ­çš„æ¨¡å‹åç§°ç”¨äºæ˜¾ç¤º"""
        model = self.config.model
        if "/" in model:
            model = model.split("/")[-1]
        # æˆªæ–­è¿‡é•¿çš„åç§°
        if len(model) > 30:
            model = model[:27] + "..."
        return model
    
    def _print_summary(self) -> None:
        """æ‰“å°è¯„ä¼°æ€»ç»“"""
        # Tokenç»Ÿè®¡
        token_usage = self.stats.total_token_usage
        cost = self.stats.total_cost
        
        mode_desc = t("éªŒè¯ç­”æ¡ˆ") if self.config.eval_mode == EvalMode.VALIDATE else t("ç›´æ¥å›ç­”")
        summary = (
            f"\n{'='*60}\n"
            f"{t('è¯„ä¼°ç»“æœæ€»ç»“')}\n"
            f"{'='*60}\n"
            f"{t('æ¨¡å‹')}: {self.config.model}\n"
            f"Provider: {self.config.provider.value}\n"
            f"{t('è¯„ä¼°æ¨¡å¼')}: {self.config.eval_mode.value} ({mode_desc})\n"
            f"{t('é£æ ¼')}: {self.config.style.value}\n"
            f"{t('æ¨ç†')}: {self.config.use_reasoning}\n"
            f"{'='*60}\n"
            f"{t('æ€»æ ·æœ¬æ•°')}: {self.stats.total + self.stats.errors}\n"
            f"{t('æœ‰æ•ˆè§£æ')}: {self.stats.total}\n"
            f"{t('æ­£ç¡®')}: {self.stats.correct}\n"
            f"{t('é”™è¯¯')}: {self.stats.total - self.stats.correct}\n"
            f"{t('è§£æ/APIé”™è¯¯')}: {self.stats.errors}\n"
            f"{'='*60}\n"
            f"{t('å‡†ç¡®ç‡')}: {self.stats.accuracy:.2%}\n"
            f"{'='*60}\n"
            f"{t('Tokenç»Ÿè®¡')}:\n"
            f"  {t('è¾“å…¥Token')}: {token_usage.prompt_tokens:,}\n"
            f"  {t('è¾“å‡ºToken')}: {token_usage.completion_tokens:,}\n"
            f"  {t('æ€»Token')}: {token_usage.total_tokens:,}\n"
        )
        
        if token_usage.reasoning_tokens > 0:
            summary += f"  {t('æ¨ç†Token')}: {token_usage.reasoning_tokens:,}\n"
        
        summary += (
            f"{'='*60}\n"
            f"{t('æˆæœ¬ç»Ÿè®¡ (USD)')}:\n"
            f"  {t('è¾“å…¥æˆæœ¬')}: ${cost.prompt_cost:.6f}\n"
            f"  {t('è¾“å‡ºæˆæœ¬')}: ${cost.completion_cost:.6f}\n"
            f"  {t('æ€»æˆæœ¬')}: ${cost.total_cost:.6f}\n"
        )
        
        if cost.prompt_price_per_m > 0:
            summary += (
                f"  Price (per M tokens): input=${cost.prompt_price_per_m:.2f}, output=${cost.completion_price_per_m:.2f}\n"
            )
        
        summary += (
            f"{'='*60}\n"
            f"{t('LogProbsç»Ÿè®¡')}:\n"
            f"  {t('å¹³å‡LogProbs (å…¨éƒ¨)')}: {self.stats.avg_logprobs_all:.4f}\n"
            f"  {t('å¹³å‡LogProbs (æ­£ç¡®)')}: {self.stats.avg_logprobs_correct_samples:.4f}\n"
            f"  {t('å¹³å‡LogProbs (é”™è¯¯)')}: {self.stats.avg_logprobs_fail_samples:.4f}\n"
            f"{'='*60}\n"
        )
        
        self.console.print(Panel(summary, title=f"[bold green]{t('è¯„ä¼°å®Œæˆ')}[/bold green]"))
        
        # summaryå†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆä¸åœ¨æ§åˆ¶å°é‡å¤æ˜¾ç¤ºï¼‰
        self._write_to_log_file(summary)
        
        # LogProbsåˆ†å¸ƒåªè¾“å‡ºåˆ°æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°
        self._log_logprobs_distribution()
    
    def _write_to_log_file(self, content: str) -> None:
        """ç›´æ¥å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆä¸åœ¨æ§åˆ¶å°æ˜¾ç¤ºï¼‰"""
        from datetime import datetime
        log_path = self.logger_manager.log_path
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        with open(log_path, "a", encoding="utf-8") as f:
            for line in content.strip().split("\n"):
                f.write(f"{timestamp} | INFO     | Summary - {line}\n")
    
    def _log_logprobs_distribution(self) -> None:
        """å°†LogProbsåˆ†å¸ƒç›´æ¥å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆä¸åœ¨æ§åˆ¶å°æ˜¾ç¤ºï¼‰"""
        import io
        import sys
        from datetime import datetime
        
        # ç›´æ¥å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œä¸é€šè¿‡loggerï¼ˆé¿å…è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
        log_path = self.logger_manager.log_path
        
        def capture_output(func, *args, **kwargs) -> str:
            """æ•è·å‡½æ•°çš„stdoutè¾“å‡º"""
            old_stdout = sys.stdout
            sys.stdout = io.StringIO()
            try:
                func(*args, **kwargs)
                return sys.stdout.getvalue()
            finally:
                sys.stdout = old_stdout
        
        def write_to_log(content: str) -> None:
            """ç›´æ¥å†™å…¥æ—¥å¿—æ–‡ä»¶"""
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
            with open(log_path, "a", encoding="utf-8") as f:
                for line in content.strip().split("\n"):
                    f.write(f"{timestamp} | INFO     | LogProbsåˆ†å¸ƒ - {line}\n")
        
        if self.stats.avg_logprobs_list:
            write_to_log("\n=== LogProbsåˆ†å¸ƒ (å…¨éƒ¨) ===")
            output = capture_output(StatisticsHelper.print_distribution_summary, self.stats.avg_logprobs_list)
            write_to_log(output)
            output = capture_output(StatisticsHelper.print_text_histogram_quantile, self.stats.avg_logprobs_list, 15, "â–ˆ", 80)
            write_to_log(output)
        
        if self.stats.avg_logprobs_list_correct:
            write_to_log("\n=== LogProbsåˆ†å¸ƒ (æ­£ç¡®) ===")
            output = capture_output(StatisticsHelper.print_distribution_summary, self.stats.avg_logprobs_list_correct)
            write_to_log(output)
            output = capture_output(StatisticsHelper.print_text_histogram_quantile, self.stats.avg_logprobs_list_correct, 15, "â–’", 80)
            write_to_log(output)
        
        if self.stats.avg_logprobs_list_fail:
            write_to_log("\n=== LogProbsåˆ†å¸ƒ (é”™è¯¯) ===")
            output = capture_output(StatisticsHelper.print_distribution_summary, self.stats.avg_logprobs_list_fail)
            write_to_log(output)
            output = capture_output(StatisticsHelper.print_text_histogram_quantile, self.stats.avg_logprobs_list_fail, 15, "â–“", 80)
            write_to_log(output)


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="BoolQ LLMè¯„ä¼°å®éªŒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨OpenRouterè¯„ä¼°Gemini
  python main.py --model google/gemini-2.0-flash-001 --style sse --limit 100

  # ä½¿ç”¨ç«å±±å¼•æ“è¯„ä¼°DeepSeek
  python main.py --model deepseek-v3-250324 --style direct --concurrency 20

  # ä½¿ç”¨Vertex AIè¯„ä¼°Gemini (Geminiæ¨¡å‹è‡ªåŠ¨ä½¿ç”¨Vertex AI)
  python main.py --model gemini-2.5-flash --style json --reasoning
        """
    )
    
    parser.add_argument(
        "--model", "-m",
        type=str,
        default="google/gemini-2.0-flash-001",
        help="æ¨¡å‹åç§° (é»˜è®¤: google/gemini-2.0-flash-001)"
    )
    
    parser.add_argument(
        "--style", "-s",
        type=str,
        choices=["direct", "sse", "json"],
        required=True,
        help="æç¤ºè¯å’Œè§£æé£æ ¼"
    )
    
    parser.add_argument(
        "--eval-mode", "-e",
        type=str,
        choices=["validate", "answer"],
        default="validate",
        help="è¯„ä¼°æ¨¡å¼: validate=éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§, answer=ç›´æ¥å›ç­”é—®é¢˜ (é»˜è®¤: validate)"
    )
    
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=0,
        help="é™åˆ¶æ•°æ®æ¡æ•°, 0è¡¨ç¤ºå…¨éƒ¨ (é»˜è®¤: 0)"
    )
    
    parser.add_argument(
        "--reasoning",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ¨ç†"
    )
    
    parser.add_argument(
        "--split",
        type=str,
        default="validation",
        choices=["train", "validation", "all"],
        help="æ•°æ®é›†åˆ†å‰²: train, validation, æˆ– all (åŒæ—¶åŠ è½½ä¸¤è€…) (é»˜è®¤: validation)"
    )
    
    parser.add_argument(
        "--reversal", "-r",
        type=float,
        default=0.3,
        help="ç­”æ¡ˆåè½¬æ¯”ä¾‹ (é»˜è®¤: 0.3)"
    )
    
    parser.add_argument(
        "--reason-order",
        type=str,
        default="reason-after",
        choices=["reason-first", "reason-after"],
        help="æ¨ç†é¡ºåº (é»˜è®¤: reason-after)"
    )
    
    parser.add_argument(
        "--concurrency", "-c",
        type=int,
        default=10,
        help="æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 10)"
    )
    
    parser.add_argument(
        "--lang",
        type=str,
        choices=["zh", "en"],
        default=None,
        help="è¯­è¨€ (zh=ä¸­æ–‡, en=English), ä¹Ÿå¯è®¾ç½®ç¯å¢ƒå˜é‡ BOOLQ_LANG"
    )
    
    return parser.parse_args()


async def main() -> None:
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®è¯­è¨€
    if args.lang:
        set_language(args.lang)
    
    # åˆ›å»ºé…ç½®
    config = ExperimentConfig.from_args(args)
    
    # åˆ›å»ºConsole (åœ¨é…ç½®ååˆ›å»ºï¼Œç¡®ä¿æ—¥å¿—ä½¿ç”¨åŒä¸€ä¸ªconsole)
    console = Console()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    mode_desc = t("éªŒè¯ç­”æ¡ˆ") if config.eval_mode == EvalMode.VALIDATE else t("ç›´æ¥å›ç­”")
    reversal_info = f"\n[bold]{t('åè½¬æ¯”ä¾‹')}:[/bold] {config.reversal_ratio}" if config.eval_mode == EvalMode.VALIDATE else ""
    console.print(Panel(
        f"[bold]{t('æ¨¡å‹')}:[/bold] {config.model}\n"
        f"[bold]Provider:[/bold] {config.provider.value}\n"
        f"[bold]{t('è¯„ä¼°æ¨¡å¼')}:[/bold] {config.eval_mode.value} ({mode_desc})\n"
        f"[bold]{t('é£æ ¼')}:[/bold] {config.style.value}\n"
        f"[bold]{t('æ¨ç†')}:[/bold] {config.use_reasoning}\n"
        f"[bold]{t('æ•°æ®é›†')}:[/bold] {config.split} (limit={config.limit})\n"
        f"[bold]{t('å¹¶å‘æ•°')}:[/bold] {config.concurrency}{reversal_info}",
        title=f"[bold cyan]{t('BoolQè¯„ä¼°é…ç½®')}[/bold cyan]"
    ))
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œ
    evaluator = BoolQEvaluator(config, console)
    
    try:
        await evaluator.run()
    except KeyboardInterrupt:
        console.print(f"\n[yellow]{t('è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­')}[/yellow]")
    except Exception as e:
        console.print(f"\n[red]è¯„ä¼°å¤±è´¥: {e}[/red]")
        logger.exception("è¯„ä¼°å¼‚å¸¸")
        raise


if __name__ == "__main__":
    asyncio.run(main())
