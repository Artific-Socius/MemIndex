"""
MemIndex Batch Runner - æ‰¹é‡è¿è¡Œå…¥å£

æ”¯æŒå¤šä¸ªé…ç½®å¹¶è¡Œè¿è¡ŒåŸºå‡†æµ‹è¯•ã€‚
æ‰€æœ‰ä»»åŠ¡é…ç½®åœ¨ä¸€ä¸ª batch_config.yaml æ–‡ä»¶ä¸­ã€‚

ä¸ main.py çš„åŒºåˆ«:
    - main.py: å•ä»»åŠ¡è¿è¡Œï¼Œé€‚åˆè°ƒè¯•å’Œå•æ¬¡æµ‹è¯•
    - batch_main.py: æ‰¹é‡å¹¶è¡Œè¿è¡Œï¼Œé€‚åˆå¤§è§„æ¨¡å¯¹æ¯”å®éªŒ

æ ¸å¿ƒç‰¹æ€§:
    - å¹¶è¡Œæ‰§è¡Œï¼šé€šè¿‡ max_parallel æ§åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°
    - ä»»åŠ¡éš”ç¦»ï¼šæ¯ä¸ªä»»åŠ¡æœ‰ç‹¬ç«‹çš„ LLM æ§åˆ¶å™¨å’Œè´¹ç”¨è¿½è¸ª
    - å®æ—¶æ˜¾ç¤ºï¼šä½¿ç”¨ Rich æ˜¾ç¤ºå¤šä»»åŠ¡è¿›åº¦
    - ä¼˜é›…ä¸­æ–­ï¼šæ”¯æŒ Ctrl+C ä¸­æ–­ï¼Œå·²å®Œæˆä»»åŠ¡ä¼šä¿å­˜
"""

from __future__ import annotations

import argparse
import asyncio
import os
import time
import traceback
from pathlib import Path
from typing import List, Dict, Any

from loguru import logger
from rich.panel import Panel

from config import Config, ConfigManager
from config.batch_config import BatchConfig, BatchConfigManager, TaskConfig
from utils import load_dataset, setup_logging, get_console
from utils.litellm_controller import LiteLLMController, CostTracker
from utils.task_display import MultiTaskDisplay, TaskStatus
from core import Actuator, Runner, Report
from prompts import get_prompt_manager

# è·å– MemIndex æ¨¡å—çš„æ ¹ç›®å½•
MEMINDEX_ROOT = os.path.dirname(os.path.abspath(__file__))


def resolve_path(path: str) -> str:
    """
    è§£æè·¯å¾„ï¼Œå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    
    Args:
        path: åŸå§‹è·¯å¾„
        
    Returns:
        è§£æåçš„ç»å¯¹è·¯å¾„
    """
    if os.path.isabs(path):
        return path
    return os.path.normpath(os.path.join(MEMINDEX_ROOT, path))


class BatchRunner:
    """
    æ‰¹é‡è¿è¡Œå™¨
    
    ç®¡ç†å¤šä¸ªä»»åŠ¡çš„å¹¶è¡Œæ‰§è¡Œï¼Œæä¾›ï¼š
    - ä¿¡å·é‡æ§åˆ¶å¹¶è¡Œæ•°
    - ç‹¬ç«‹çš„è´¹ç”¨è¿½è¸ª
    - å®æ—¶è¿›åº¦æ˜¾ç¤º
    - é”™è¯¯å¤„ç†å’Œä¸­æ–­æ”¯æŒ
    
    å·¥ä½œæµç¨‹:
        1. æ³¨å†Œæ‰€æœ‰ä»»åŠ¡åˆ°æ˜¾ç¤ºå™¨
        2. é€šè¿‡ä¿¡å·é‡æ§åˆ¶å¹¶è¡Œæ•°
        3. ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ LLM æ§åˆ¶å™¨
        4. æ‰§è¡Œä»»åŠ¡å¹¶å®æ—¶æ›´æ–°è¿›åº¦
        5. ç”ŸæˆæŠ¥å‘Šå¹¶ä¿å­˜ç»“æœ
    """
    
    def __init__(
        self,
        batch_config: BatchConfig,
        system_config: Config,
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡è¿è¡Œå™¨
        
        Args:
            batch_config: æ‰¹é‡è¿è¡Œé…ç½®ï¼ˆåŒ…å«æ‰€æœ‰ä»»åŠ¡å®šä¹‰ï¼‰
            system_config: ç³»ç»Ÿé…ç½®ï¼ˆLLM providers ç­‰ï¼‰
        """
        self.batch_config = batch_config
        self.system_config = system_config
        self.display = MultiTaskDisplay(get_console())  # å¤šä»»åŠ¡è¿›åº¦æ˜¾ç¤ºå™¨
        self.results: Dict[str, Any] = {}               # å­˜å‚¨å„ä»»åŠ¡çš„ç»“æœ
        self._semaphore: asyncio.Semaphore = None       # æ§åˆ¶å¹¶è¡Œæ•°çš„ä¿¡å·é‡
        
        # åˆå§‹åŒ– PromptManagerï¼ˆæ‰€æœ‰ä»»åŠ¡å…±äº«ï¼‰
        self.prompt_manager = get_prompt_manager(resolve_path("prompts/prompts.yaml"))
    
    async def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œæ‰€æœ‰ä»»åŠ¡
        
        è¿™æ˜¯æ‰¹é‡è¿è¡Œçš„ä¸»å…¥å£ï¼Œè´Ÿè´£ï¼š
        1. åˆ›å»ºå¹¶è¡Œæ§åˆ¶ä¿¡å·é‡
        2. æ³¨å†Œæ‰€æœ‰ä»»åŠ¡
        3. å¹¶å‘æ‰§è¡Œä»»åŠ¡
        4. å¤„ç†ä¸­æ–­å’Œé”™è¯¯
        
        Returns:
            ä»»åŠ¡ç»“æœå­—å…¸ {task_id: result}
        """
        tasks = self.batch_config.tasks
        
        if not tasks:
            logger.warning("No tasks found in config")
            return {}
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶è¡Œæ•°ï¼Œé™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°é‡
        self._semaphore = asyncio.Semaphore(self.batch_config.max_parallel)
        self._interrupted = False    # æ˜¯å¦è¢«ä¸­æ–­
        self._async_tasks: list[asyncio.Task] = []  # å¼‚æ­¥ä»»åŠ¡åˆ—è¡¨
        
        # æ³¨å†Œæ‰€æœ‰ä»»åŠ¡åˆ°æ˜¾ç¤ºå™¨ï¼ˆæ˜¾ç¤ºä»»åŠ¡åˆ—è¡¨å’Œé¢„ä¼°æ­¥æ•°ï¼‰
        for i, task_config in enumerate(tasks):
            task_id = f"task_{i}"
            total_steps = self._estimate_total_steps(task_config)
            self.display.register_task(
                task_id, 
                task_config.name, 
                total_steps,
                chat_model=task_config.chat_model,
            )
        
        # å¯åŠ¨è¿›åº¦æ˜¾ç¤º
        self.display.start()
        
        try:
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            for i, task_config in enumerate(tasks):
                task_id = f"task_{i}"
                async_task = asyncio.create_task(
                    self._run_single_task(task_id, task_config)
                )
                self._async_tasks.append(async_task)
                
                # ä»»åŠ¡é—´å»¶è¿Ÿï¼ˆé¿å…åŒæ—¶å¯åŠ¨é€ æˆçš„èµ„æºç«äº‰ï¼‰
                if self.batch_config.task_delay > 0 and i < len(tasks) - 1:
                    await asyncio.sleep(self.batch_config.task_delay)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆreturn_exceptions=True ç¡®ä¿ä¸€ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡ï¼‰
            await asyncio.gather(*self._async_tasks, return_exceptions=True)
        
        except asyncio.CancelledError:
            # å¤„ç† Ctrl+C ä¸­æ–­
            self._interrupted = True
            # å–æ¶ˆæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            for task in self._async_tasks:
                if not task.done():
                    task.cancel()
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå–æ¶ˆ
            await asyncio.gather(*self._async_tasks, return_exceptions=True)
            # æ›´æ–°è¿è¡Œä¸­ä»»åŠ¡çš„çŠ¶æ€ä¸ºå–æ¶ˆ
            for task_state in self.display.tasks.values():
                if task_state.status == TaskStatus.RUNNING:
                    task_state.status = TaskStatus.CANCELLED
                    task_state.end_time = time.time()
        
        finally:
            # åœæ­¢è¿›åº¦æ˜¾ç¤º
            self.display.stop()
            
            # æ‰“å°æœ€ç»ˆæ‘˜è¦ï¼ˆåŒ…å«æ‰€æœ‰ä»»åŠ¡çš„ç»“æœç»Ÿè®¡ï¼‰
            self.display.print_final_summary(interrupted=self._interrupted)
        
        return self.results
    
    def _estimate_total_steps(self, task_config: TaskConfig) -> int:
        """
        ä¼°ç®—ä»»åŠ¡çš„æ€»æ­¥æ•°
        
        é€šè¿‡é¢„åŠ è½½æ•°æ®é›†æ¥è®¡ç®—æ€»æ­¥æ•°ï¼Œç”¨äºè¿›åº¦æ¡æ˜¾ç¤ºã€‚
        
        Args:
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            é¢„ä¼°çš„æ€»æ­¥æ•°
        """
        try:
            benchmark_config_path = resolve_path(task_config.benchmark_config)
            if os.path.exists(benchmark_config_path):
                dataset = load_dataset(benchmark_config_path)
                # æ€»æ­¥æ•° = æ‰€æœ‰æµ‹è¯•åºåˆ—çš„é¡¹æ•°ä¹‹å’Œ
                return sum(len(seq.items) for seq in dataset.data.values())
        except Exception:
            pass
        return 100  # é»˜è®¤ä¼°ç®—å€¼
    
    async def _run_single_task(self, task_id: str, task_config: TaskConfig) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªä»»åŠ¡
        
        è¿™æ˜¯æ¯ä¸ªå¹¶è¡Œä»»åŠ¡çš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ï¼š
        1. è·å–ä¿¡å·é‡ï¼ˆæ§åˆ¶å¹¶è¡Œæ•°ï¼‰
        2. åˆ›å»ºç‹¬ç«‹çš„è´¹ç”¨è¿½è¸ªå™¨
        3. æ‰§è¡Œä»»åŠ¡
        4. æ›´æ–°çŠ¶æ€å’Œå¤„ç†é”™è¯¯
        
        Args:
            task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            ä»»åŠ¡ç»“æœå­—å…¸
        """
        # é€šè¿‡ä¿¡å·é‡æ§åˆ¶å¹¶è¡Œæ•°ï¼ˆasync with è‡ªåŠ¨è·å–å’Œé‡Šæ”¾ï¼‰
        async with self._semaphore:
            self.display.update_task(task_id, status=TaskStatus.RUNNING)
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„è´¹ç”¨è¿½è¸ªå™¨ï¼ˆé¿å…å¤šä»»åŠ¡é—´çš„è´¹ç”¨æ··æ·†ï¼‰
            task_cost_tracker = CostTracker()
            
            try:
                # æ‰§è¡Œä»»åŠ¡æ ¸å¿ƒé€»è¾‘
                result = await self._execute_task(task_id, task_config, task_cost_tracker)
                self.results[task_id] = result
                
                # æ›´æ–°æœ€ç»ˆçŠ¶æ€ä¸ºå®Œæˆ
                self.display.update_task(
                    task_id,
                    status=TaskStatus.COMPLETED,
                    cost=task_cost_tracker.total_cost,
                )
                
                return result
            
            except Exception as e:
                # è¯¦ç»†é”™è¯¯æ—¥å¿—
                error_msg = str(e)
                error_traceback = traceback.format_exc()
                logger.error(f"Task {task_config.name} failed: {error_msg}")
                logger.error(f"Task config: model={task_config.chat_model}, provider={task_config.memory_provider}, eval_mode={task_config.eval_mode}")
                logger.error(f"Full traceback:\n{error_traceback}")
                
                self.display.update_task(
                    task_id,
                    status=TaskStatus.FAILED,
                    error=error_msg,
                )
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡
                if not self.batch_config.continue_on_error:
                    raise
                
                return {"error": str(e)}
    
    async def _execute_task(
        self,
        task_id: str,
        task_config: TaskConfig,
        cost_tracker: CostTracker,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡çš„æ ¸å¿ƒé€»è¾‘
        
        è¿™æ˜¯ä»»åŠ¡æ‰§è¡Œçš„å®é™…å®ç°ï¼Œä¸ main.py ä¸­çš„ main() å‡½æ•°ç±»ä¼¼ï¼Œ
        ä½†å¢åŠ äº†è¿›åº¦å›è°ƒå’Œé™é»˜æ¨¡å¼æ”¯æŒã€‚
        
        Args:
            task_id: ä»»åŠ¡æ ‡è¯†
            task_config: ä»»åŠ¡é…ç½®
            cost_tracker: è´¹ç”¨è¿½è¸ªå™¨
            
        Returns:
            ä»»åŠ¡ç»“æœï¼ˆåŒ…å«è€—æ—¶ã€tokenã€è´¹ç”¨ç­‰ï¼‰
        """
        # å°† TaskConfig è½¬æ¢ä¸º RunningConfigï¼ˆç»Ÿä¸€é…ç½®æ ¼å¼ï¼‰
        running_config = task_config.to_running_config()
        
        # è§£æè·¯å¾„
        benchmark_config_path = resolve_path(running_config.benchmark_config)
        report_dir_path = resolve_path(running_config.report_dir)
        
        # æ›´æ–°æ˜¾ç¤º
        self.display.update_task(task_id, log_message=f"Model: {running_config.chat_model}")
        
        # ========== åˆ›å»ºä»»åŠ¡ç‹¬ç«‹çš„ LLM æ§åˆ¶å™¨ ==========
        llm_controller = LiteLLMController(
            env_file=resolve_path(".env"),
            retry_times=self.system_config.llm_config.llm_retry_times,
            track_cost=True,
        )
        # ä½¿ç”¨ä»»åŠ¡ä¸“å±çš„è´¹ç”¨è¿½è¸ªå™¨ï¼ˆå…³é”®ï¼šç¡®ä¿è´¹ç”¨ç»Ÿè®¡ç‹¬ç«‹ï¼‰
        llm_controller.cost_tracker = cost_tracker
        await llm_controller._init_provider()
        
        # ========== åŠ è½½ Agent ==========
        self.display.update_task(task_id, log_message="Loading Agent...")
        agent = self._load_agent(
            running_config, 
            llm_controller,
            chat_prompt_key=running_config.chat_prompt,
        )
        
        # ========== åŠ è½½æ•°æ®é›† ==========
        self.display.update_task(task_id, log_message="Loading dataset...")
        benchmark_dataset = load_dataset(benchmark_config_path)
        
        # ========== åˆ›å»ºæ‰§è¡Œå™¨ ==========
        actuators = {}
        for name, sequence in benchmark_dataset.data.items():
            actuator = Actuator(
                data=sequence.items,
                llm_controller=llm_controller,
                agent=agent,
                eval_model=running_config.eval_model,
                prompt_manager=self.prompt_manager,
                eval_prompt_key=running_config.eval_prompt,
                eval_mode=running_config.eval_mode,
            )
            actuator.name = name
            actuators[name] = actuator
        
        # ========== åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•° ==========
        # è¿™ä¸ªå›è°ƒå‡½æ•°ä¼šåœ¨æ¯ä¸ªæ­¥éª¤åè¢«è°ƒç”¨ï¼Œç”¨äºæ›´æ–°æ˜¾ç¤º
        def progress_callback(
            current_step: int, 
            total_steps: int, 
            tokens: int, 
            cost: float,
            actuator_name: str = "",
            actuator_step: int = 0,
            actuator_total: int = 0,
            last_action: str = "",
        ):
            # æ˜¾ç¤ºå½“å‰æ“ä½œçš„é¢„è§ˆ
            if last_action:
                action_preview = last_action[:35] + "..." if len(last_action) > 35 else last_action
                self.display.update_task(task_id, log_message=f"ğŸ’¬ {action_preview}")
            
            # æ›´æ–°è¿›åº¦ä¿¡æ¯ï¼ˆåŒ…å«å»¶è¿Ÿç»Ÿè®¡ï¼‰
            self.display.update_task(
                task_id,
                current_step=current_step,
                current_tokens=tokens,
                cost=cost_tracker.total_cost,
                current_actuator=actuator_name,
                current_actuator_step=actuator_step,
                total_actuator_steps=actuator_total,
                # å»¶è¿Ÿç»Ÿè®¡ï¼ˆä» agent è·å–ï¼‰
                has_memory_backend=agent.has_memory_backend,
                avg_memory_latency=agent.avg_memory_latency,
                avg_chat_latency=agent.avg_chat_latency,
                last_memory_latency=agent.last_memory_latency,
                last_chat_latency=agent.last_chat_latency,
            )
            self.display.refresh()
        
        # ========== åˆ›å»º Runnerï¼ˆé™é»˜æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºè‡ªå·±çš„è¿›åº¦æ¡ï¼‰==========
        runner = Runner(
            actuators=list(actuators.values()),
            nonsense=benchmark_dataset.nonsense_list,
            head_prompts=benchmark_dataset.head_prompts,
            agent=agent,
            memory_distance=benchmark_dataset.memory_distance,
            eval_model=running_config.eval_model,
            show_progress=False,           # ä¸æ˜¾ç¤º Runner è‡ªå·±çš„è¿›åº¦æ¡
            progress_callback=progress_callback,  # ä½¿ç”¨å›è°ƒæ›´æ–°æ‰¹é‡è¿è¡Œå™¨çš„æ˜¾ç¤º
            silent=True,                   # é™é»˜æ¨¡å¼
        )
        
        # æ›´æ–°æ€»æ­¥æ•°ï¼ˆä½¿ç”¨å®é™…è®¡ç®—çš„å€¼æ›¿ä»£ä¼°ç®—å€¼ï¼‰
        total_steps = sum(len(a.data) for a in actuators.values())
        self.display.tasks[task_id].total_steps = total_steps
        
        # ========== è¿è¡Œæµ‹è¯• ==========
        self.display.update_task(task_id, log_message="Running benchmark...")
        time_start = time.time()
        await runner.run()  # æ‰§è¡Œæµ‹è¯•
        time_end = time.time()
        
        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        if not os.path.exists(report_dir_path):
            os.makedirs(report_dir_path)
        
        # ========== ç”ŸæˆæŠ¥å‘Š ==========
        self.display.update_task(task_id, log_message="Generating report...")
        
        benchmark_name = (
            f"{running_config.benchmark_config.split('/')[-1].replace('.', '_')}-"
            f"{running_config.memory_provider.replace('/', '_')}-"
            f"{running_config.chat_model.replace('/', '_')}-"
            f"{running_config.eval_mode}"
        )
        
        try:
            report = Report(
                report_path=report_dir_path,
                config_path=benchmark_config_path,
                time_start=time_start,
                time_end=time_end,
                runner=runner,
                actuator_names=list(actuators.keys()),
                agent=running_config.memory_provider,
                benchmark_name=benchmark_name,
                full_tokens=runner.current_tokens,
                model=running_config.chat_model,
                extra_metadata=agent.extra_metadata,
                eval_mode=running_config.eval_mode,
                chat_prompt=running_config.chat_prompt,
                eval_prompt=running_config.eval_prompt,
            )
            report.save()
        except Exception as e:
            logger.error(f"Failed to generate report: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
        
        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        self.display.update_task(
            task_id,
            current_step=total_steps,
            current_tokens=runner.current_tokens,
            cost=cost_tracker.total_cost,
            log_message="Complete!",
        )
        
        # è¿”å›ä»»åŠ¡ç»“æœ
        return {
            "name": task_config.name,
            "elapsed_time": time_end - time_start,
            "tokens": runner.current_tokens,
            "cost": cost_tracker.total_cost,
            "report_path": report_dir_path,
        }
    
    def _load_agent(
        self, 
        running_config, 
        llm_controller: LiteLLMController,
        chat_prompt_key: str = None,
    ):
        """
        åŠ è½½ Agent
        
        æ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”ç±»å‹çš„ Agent å®ä¾‹ã€‚
        
        Args:
            running_config: è¿è¡Œé…ç½®
            llm_controller: LLM æ§åˆ¶å™¨
            chat_prompt_key: Chat æç¤ºè¯ key
            
        Returns:
            Agent å®ä¾‹
        """
        agent_type = running_config.memory_provider
        
        if agent_type == "llm":
            from components.agents import LLMAgent
            return LLMAgent(
                llm_controller=llm_controller,
                model=running_config.chat_model,
                context_window=running_config.context_window,
            )
        elif agent_type == "memecho":
            from components.agents import MemechoAgent
            return MemechoAgent(
                llm_controller=llm_controller,
                model=running_config.chat_model,
                context_window=running_config.context_window,
            )
        elif agent_type == "example":
            from components.agents import ExampleAgent
            return ExampleAgent()
        elif agent_type == "mem0":
            from components.agents import Mem0Agent
            return Mem0Agent(
                llm_controller=llm_controller,
                model=running_config.chat_model,
                context_window=running_config.context_window,
                prompt_manager=self.prompt_manager,
                chat_prompt_key=chat_prompt_key,
            )
        elif agent_type == "mem0_graph":
            from components.agents import Mem0GraphAgent
            return Mem0GraphAgent(
                llm_controller=llm_controller,
                model=running_config.chat_model,
                context_window=running_config.context_window,
                prompt_manager=self.prompt_manager,
                chat_prompt_key=chat_prompt_key,
            )
        else:
            raise NotImplementedError(f"Unknown agent type: {agent_type}")


async def main(args) -> None:
    """
    æ‰¹é‡è¿è¡Œä¸»å‡½æ•°
    
    è´Ÿè´£åŠ è½½é…ç½®ã€åˆ›å»º BatchRunner å¹¶å¯åŠ¨æ‰§è¡Œã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    console = get_console()
    
    # è§£æé…ç½®æ–‡ä»¶è·¯å¾„
    config_path = resolve_path(args.config)
    batch_config_path = resolve_path(args.batch_config)
    
    # åŠ è½½ç³»ç»Ÿé…ç½®
    config_manager = ConfigManager[Config](config_path, Config)
    system_config = config_manager.get_config()
    
    # åŠ è½½æ‰¹é‡ä»»åŠ¡é…ç½®
    batch_manager = BatchConfigManager(batch_config_path)
    batch_config = batch_manager.load_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.max_parallel:
        batch_config.max_parallel = args.max_parallel
    
    if not batch_config.tasks:
        logger.error(f"No tasks found in {batch_config_path}")
        return
    
    # æ˜¾ç¤ºä»»åŠ¡åˆ—è¡¨æ¦‚è§ˆ
    logger.info(f"Found {len(batch_config.tasks)} tasks:")
    for task in batch_config.tasks:
        logger.info(f"  - {task.name}: {task.chat_model}")
    
    console.print()
    
    # åˆ›å»ºå¹¶è¿è¡Œæ‰¹é‡è¿è¡Œå™¨
    batch_runner = BatchRunner(
        batch_config=batch_config,
        system_config=system_config,
    )
    
    await batch_runner.run()


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        è§£æåçš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(description="Run Batch Benchmark.")
    
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="System Config (LLM providers)"
    )
    
    parser.add_argument(
        "--batch_config",
        type=str,
        default="batch_config.yaml",
        help="Batch config file (contains all task configs)"
    )
    
    parser.add_argument(
        "--max_parallel",
        type=int,
        default=None,
        help="Maximum number of parallel tasks (overrides config)"
    )
    
    parser.add_argument(
        "--log_level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Log level"
    )
    
    parser.add_argument(
        "--list",
        action="store_true",
        dest="list_tasks",
        help="List all tasks without executing (check configuration)"
    )
    
    return parser.parse_args()


def list_tasks(args) -> None:
    """
    åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡é…ç½®ï¼ˆä¸æ‰§è¡Œï¼‰
    
    ç”¨äºæ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚
    ä½¿ç”¨ --list å‚æ•°è§¦å‘ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    from rich.table import Table
    
    console = get_console()
    batch_config_path = resolve_path(args.batch_config)
    
    try:
        batch_manager = BatchConfigManager(batch_config_path)
        batch_config, all_tasks = batch_manager.load_all_tasks()
    except FileNotFoundError as e:
        console.print(f"[red]Error:[/red] {e}")
        return
    except Exception as e:
        console.print(f"[red]Configuration Error:[/red] {e}")
        return
    
    # æ˜¾ç¤ºå…¨å±€é…ç½®
    console.print(Panel(
        f"[cyan]Max Parallel:[/cyan] {batch_config.max_parallel}\n"
        f"[cyan]Continue on Error:[/cyan] {batch_config.continue_on_error}\n"
        f"[cyan]Task Delay:[/cyan] {batch_config.task_delay}s\n"
        f"[cyan]Default Chat Prompt:[/cyan] {batch_config.default_chat_prompt or '(default)'}\n"
        f"[cyan]Default Eval Prompt:[/cyan] {batch_config.default_eval_prompt or '(default)'}\n"
        f"[cyan]Default Eval Mode:[/cyan] {batch_config.default_eval_mode}",
        title="[bold blue]Global Settings[/bold blue]",
        border_style="blue",
    ))
    console.print()
    
    # åˆ›å»ºä»»åŠ¡è¡¨æ ¼
    table = Table(title="Task Configurations", show_header=True, header_style="bold magenta")
    table.add_column("#", style="dim", width=3)
    table.add_column("Status", justify="center", width=8)
    table.add_column("Name", style="cyan", min_width=20)
    table.add_column("Agent", style="yellow", width=10)
    table.add_column("Model", style="green", min_width=25)
    table.add_column("Eval Mode", style="blue", width=8)
    table.add_column("Context", justify="right", width=8)
    table.add_column("Prompts", style="dim", width=15)
    table.add_column("Dataset", style="dim")
    
    enabled_count = 0
    for i, task in enumerate(all_tasks):
        # æ˜¾ç¤ºå¯ç”¨/ç¦ç”¨çŠ¶æ€
        if task.enabled:
            status = "[green]âœ“ ON[/green]"
            enabled_count += 1
        else:
            status = "[dim]âœ— OFF[/dim]"
        
        # ç®€åŒ–æ˜¾ç¤º
        model_short = task.chat_model.split("/")[-1] if "/" in task.chat_model else task.chat_model
        dataset_short = task.benchmark_config.split("/")[-1]
        
        # Prompt æ˜¾ç¤º
        chat_p = task.chat_prompt or "-"
        eval_p = task.eval_prompt or "-"
        prompts_str = f"C:{chat_p}/E:{eval_p}"
        
        # Eval Mode æ˜¾ç¤ºï¼ˆç”¨å›¾æ ‡åŒºåˆ†ï¼‰
        eval_mode_display = "ğŸ¯" if task.eval_mode == "binary" else "ğŸ“Š"
        
        table.add_row(
            str(i + 1),
            status,
            task.name,
            task.memory_provider,
            model_short,
            eval_mode_display,
            str(task.context_window),
            prompts_str,
            dataset_short,
        )
    
    console.print(table)
    console.print()
    
    # æ‘˜è¦ç»Ÿè®¡
    disabled_count = len(all_tasks) - enabled_count
    summary = f"[bold]Total:[/bold] {len(all_tasks)} tasks"
    if enabled_count > 0:
        summary += f"  [green]âœ“ {enabled_count} enabled[/green]"
    if disabled_count > 0:
        summary += f"  [dim]âœ— {disabled_count} disabled[/dim]"
    
    console.print(summary)
    
    if enabled_count == 0:
        console.print("[yellow]âš  No tasks enabled. Nothing will run.[/yellow]")


def run():
    """
    ç¨‹åºè¿è¡Œå…¥å£
    
    è´Ÿè´£åˆå§‹åŒ–ã€åˆ†å‘åˆ°ä¸åŒæ¨¡å¼ï¼ˆlist/runï¼‰ã€å¤„ç†ä¸­æ–­ã€‚
    """
    args = parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—
    setup_logging(level=args.log_level)
    
    # æ˜¾ç¤ºå¯åŠ¨ Banner
    console = get_console()
    console.print()
    console.print(Panel(
        "[bold cyan]MemIndex Batch Runner[/bold cyan] - Parallel Benchmark Execution",
        border_style="cyan",
    ))
    console.print()
    
    # å¦‚æœæ˜¯ --list æ¨¡å¼ï¼Œåªæ˜¾ç¤ºä»»åŠ¡åˆ—è¡¨ä¸æ‰§è¡Œ
    if args.list_tasks:
        list_tasks(args)
        return
    
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        # ä¼˜é›…å¤„ç† Ctrl+C ä¸­æ–­
        console.print()
        console.print(Panel(
            "[bold yellow]âš  Batch run interrupted by user (Ctrl+C)[/bold yellow]\n"
            "[dim]Tasks in progress have been stopped. Completed tasks are saved.[/dim]",
            title="[yellow]Interrupted[/yellow]",
            border_style="yellow",
        ))


if __name__ == "__main__":
    run()
